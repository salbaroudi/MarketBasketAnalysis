{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "After experimenting with the **mlxtend** library and investigating different metrics for itemset rules, we can now give a basic outline of how to use our rule metrics to identify top rules, for store placement or sales campaigns. \n",
    "\n",
    "In general, itemsets should be found with threshold'ed **support**, positive/negative relationships are identified with **lift**, and the strength of the said rules are measured by **conviction**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose and Goal:\n",
    "\n",
    "What is the goal of Market Basket Analysis? \n",
    "\n",
    "It is to identify reliable, *non-obvious* relationships between itemsets. In the context of transactional/sales data, we are looking for rules to point out non-obvious relationships between goods and services, that we can push to exploit hidden streams of revenue.\n",
    "\n",
    "We use the Apriori algorithm to identify itemsets with sufficient support (read: enough examples to be statistically relevent), and then find the top association rules between itemsets according to objective measures (lift and conviction etc). \n",
    "\n",
    "Once our top rules are found, we apply context and human intuition to judge if the rules are worthwhile. For example, if we derive the following two rules:\n",
    "\n",
    "$$\\{ Coffee \\} \\rightarrow \\{ Sugar, Milk \\}$$\n",
    "\n",
    "\n",
    "$$\\{ Pork,Beef,Chicken \\} \\rightarrow \\{ Lemon\\;\\; Juice \\} $$\n",
    "\n",
    "The former is obvious, and can be discarded. The latter might be because of a current cooking or health-trend, and should be investigated further.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criteria for Rule Selection:\n",
    "\n",
    "With all of these definitions and examples in mind, how should we select our rules? Lets recap what our 4 main metrics do:\n",
    "\n",
    "1) Support (of a Rule): Used to get some baseline statistical significance. If too low, our rule could have just occured by chance.\n",
    "\n",
    "2) Confidence: A measure of strength of implication: how likely that Y is likely to occur given X occurs in a transaction. Overestimates a conditional probability. Does not give information about the valence or kind of relationship between two itemsets.\n",
    "\n",
    "3) Lift: A measure of correlation between two itemsets. Indicates valence and strength of co-occurence (not causality).\n",
    "\n",
    "4) Conviction: Another measure of strength of implication, but one that indicates when two item-sets are indepednent (cv = 1), in addition to taking into account both the antecedent and consequent of the rule (confidence only considers the antecedent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to Find Good Rules:\n",
    "\n",
    "1) For the *mlxtend.apriori()* function, set the support threshold low enough to get a large number of rules.\n",
    "\n",
    "2) After itemsets have been mined with the apriori() algorithm, Get the top X rules based on the highest lift and conviction values, using the *mlxtend.association_rules()* function.\n",
    "\n",
    "3) Sort by lift first, and conviction second.\n",
    "\n",
    "4) Remove duplicate and circular rules; for bi-directional rules. If there are bi-directional rules between two nodes, select whichever rule has the highest conviction.  \n",
    "\n",
    "5) Display rules with pyvis functions(), and apply human/domain knowledge to pick out desired rules.\n",
    "\n",
    "The code below demonstrates this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.seanlib import *\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Generation Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation of Rules, prepping data frame\n",
    "#Signature: String -> DataFrame\n",
    "\n",
    "#Countries with more than 500 transactions. Some countries have so few transactions, that MBA is pointless.\n",
    "cities_avail = ['United Kingdom', 'France', 'Australia', 'Netherlands', 'Germany',\n",
    "       'Norway', 'EIRE', 'Switzerland', 'Spain', \n",
    "        'Portugal', 'Italy', 'Belgium', 'Channel Islands',  'Cyprus', 'Sweden']\n",
    "\n",
    "#Purpose: Given a Country, take the raw UCI dataset, and clean it up. \n",
    "def loadcleandata(cty):\n",
    "    df = pd.read_excel('./data/online_retail.xlsx')\n",
    "    df['Description'] = df['Description'].str.strip()\n",
    "    df.dropna(axis=0, subset=['InvoiceNo'], inplace=True)\n",
    "    df['InvoiceNo'] = df['InvoiceNo'].astype('str')\n",
    "    df = df[~df['InvoiceNo'].str.contains('C')]    \n",
    "    \n",
    "    basket = (df[df['Country'] == cty].groupby(['InvoiceNo', 'Description'])[\"Quantity\"])\n",
    "    basket = basket.sum().unstack().reset_index().fillna(0).set_index('InvoiceNo') \n",
    "    basket_sets = basket.applymap(encode_units) \n",
    "    if \"POSTAGE\" in basket_sets.columns: #Some countries have a postage itemset; it ruins our graphs and analysis.\n",
    "        basket_sets.drop('POSTAGE', inplace=True, axis=1)\n",
    "    return basket_sets\n",
    "\n",
    "#Purpose: 1-hot encoding for each feature.\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "#Signature: BasketSets, Integer, String -> DataFrame\n",
    "#Purpose: get the itemsets, and then find the rules between them, based on arguments supplied.\n",
    "def getrules(bsets,minsup,met):\n",
    "    frequent_itemsets = apriori(bsets, min_support=minsup, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=met, min_threshold=1)\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Generation Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Functions:\n",
    "def strends(x):\n",
    "    for y in x: #Frozensets...a weird datatype from our Apriori Algorithms. OK.\n",
    "        thestr = y\n",
    "    return thestr\n",
    "\n",
    "#Given a rules Data Frame, generate an html graph.\n",
    "def gen_graph(rules):\n",
    "    #reset index\n",
    "    rules.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    #make itemsets into mutable strings.\n",
    "    rules[\"antecedents\"] = rules[\"antecedents\"].apply(strends)\n",
    "    rules[\"consequents\"] = rules[\"consequents\"].apply(strends)\n",
    "    \n",
    "    #Adding nodes to our graph.\n",
    "    #I use the uniqueness aspects of sets, to make sure nodes are unique.\n",
    "    #aCSet = {-1}\n",
    "    #aCSet.remove(-1)\n",
    "    nodeDict = {}\n",
    "    for rowindex in rules.index: \n",
    "        fetch = rules.iloc[rowindex][\"antecedents\"]\n",
    "        if fetch not in nodeDict.keys():\n",
    "            nodeDict[fetch] = rowindex\n",
    "                    \n",
    "        fetch = rules.iloc[rowindex][\"consequents\"]\n",
    "        if fetch not in nodeDict.keys():\n",
    "            nodeDict[fetch] = rowindex\n",
    "        #aCSet.add(rules.iloc[rowindex][\"antecedents\"])\n",
    "        #aCSet.add(rules.iloc[rowindex][\"consequents\"])\n",
    "    \n",
    "    net = Network(width=\"800px\",height=\"800px\",notebook=True,directed=True)\n",
    "    \n",
    "    for itemset in list(nodeDict.keys()):\n",
    "        index = nodeDict[itemset]\n",
    "        net.add_node(itemset, value=rules.iloc[index][\"antecedent support\"])  \n",
    "       \n",
    "    for i, row in rules.iterrows():\n",
    "        if row[\"antecedents\"] != row[\"consequents\"]: \n",
    "            net.add_edge(source=row[\"antecedents\"],to=row[\"consequents\"],\n",
    "                        physics=False,value=row[\"conviction\"]*2,arrowStriketrhough=False)\n",
    "    \n",
    "    return net\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal Example: Use of code (Raw Data -> Final Graphical Representation): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the raw dataset:\n",
    "dfOR = pd.read_excel('./data/online_retail.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the basket_sets related to a particular country directly:\n",
    "bask_sets = loadcleandata(\"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Rules:\n",
    "df = loadcleandata(\"Germany\")\n",
    "rules = getrules(df,0.035,\"lift\") #Do you even...\n",
    "rules.sort_values(by=[\"lift\",\"conviction\"],ascending=False,inplace=True)\n",
    "rules.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules.drop(index=list(range(1,rules.shape[0],2)),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800px\"\n",
       "            height=\"800px\"\n",
       "            src=\"mygraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f979568add8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = gen_graph(rules)\n",
    "network.show(\"mygraph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and a Rule Cluster is produced! The UCI dataset has labels that don't really make sense, so we can't apply domain knowledge to pick out rules. However, it can be seen from the Cluster above, that rules with heavy set arrows would be ones to first focus on. Thank you for following this tutorial, try MBA out on your own transactional datasets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
